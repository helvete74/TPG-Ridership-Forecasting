{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57195d85-093f-46d2-8812-9f79c9e8add1",
   "metadata": {},
   "source": [
    "# Capstone proposal by PATRICK ROMAND\n",
    "\n",
    "# Predicting Passenger Ridership on TPG Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da9e81d7-c248-4bcf-8a19-f429b74a32fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_DATA_Zenodo = True\n",
    "DOWNLOAD_DATA_TPG = False\n",
    "DOWNLOAD_DATA_METEO = False\n",
    "INSTALL_LIB = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14def36-5f94-4678-ac22-038509e2e3cf",
   "metadata": {},
   "source": [
    "# 1) The problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb713e0-f53d-418d-8dea-ab472b83ae43",
   "metadata": {},
   "source": [
    "**Predicting Public Transport Ridership in Geneva Using Open Data and Machine Learning Techniques**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d598207-ccc3-41cd-a1a9-184498e785f3",
   "metadata": {},
   "source": [
    "Public transport services play a central role in sustainable urban mobility.  \n",
    "Understanding and anticipating ridership helps optimize resource planning, adjust supply to demand, and improve the user experience.  \n",
    "\n",
    "In this project, we focus on the Geneva Public Transport network (TPG).  \n",
    "The main goal is to predict **daily ridership** using historical data.  \n",
    "This forecasting task is complex because many factors influence it: weather conditions, day of the week, holiday periods, exceptional events, and more.  \n",
    "\n",
    "To better capture this complexity, we use a two-level approach:  \n",
    "\n",
    "**Global level**: predict the total daily **ridership** across the entire TPG network.  \n",
    "At this level, several Machine Learning models are compared to evaluate their performance and identify the most suitable approaches for this type of prediction.  \n",
    "\n",
    "**Local level**: focus on a specific stop in the network.  \n",
    "This part is presented as a concrete use case, showing how the model can be applied in operational situations with **passenger counts**.  \n",
    "\n",
    "This two-level approach highlights two complementary views:  \n",
    "an academic view, which compares different Machine Learning models, and a professional view, which presents a concrete use case for network management.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b863f6-4d5c-4c9e-990c-2ff9e57eb158",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2) The data\n",
    "\n",
    "## (a) Clear overview of your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e867a2b-0251-4d58-84ff-30ee7504662b",
   "metadata": {},
   "source": [
    "The project relies on several data sources:  \n",
    "\n",
    "## Main sources  \n",
    "- **TPG Open Data**  \n",
    "  Planned schedules, line and stop identifiers, GPS positions, ridership, available on the [TPG Open Data portal](https://opendata.tpg.ch/pages/accueil/).  \n",
    "\n",
    "- **Weather data**  \n",
    "  Temperature, precipitation, general conditions, obtained from weather sites such as [Open-Meteo](https://open-meteo.com/).  \n",
    "\n",
    "## Secondary sources  \n",
    "- **Calendar data**  \n",
    "  Public holidays using the *holidays* library.  \n",
    "\n",
    "- **Real-time data** (Optional)  \n",
    "  Operational real-time data available via [Open Transport Data Switzerland](https://data.opentransportdata.swiss/dataset/istdaten), not yet integrated but possible for future iterations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4465fa1d-2ed3-4bf6-8ec2-35c9e2923a5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TPG Data via API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c893316-f83a-4c2c-b20b-0f0506f9f09d",
   "metadata": {
    "tags": []
   },
   "source": [
    "From the [TPG Open Data portal](https://opendata.tpg.ch/pages/accueil/), data will be collected using the available API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf4bbe27-902a-4c79-a96b-7c6093b26e13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96ba2ce7-12c2-4642-8262-7b5278cf2403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_url  = \"https://tpg.opendatasoft.com/api/explore/v2.1\"\n",
    "date_max  = \"2025-06-22T23:59:59\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e6a36b-e5a0-4905-a4fa-593d860f2295",
   "metadata": {},
   "source": [
    "First, we need the list of available datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3882c70d-5486-4866-803a-cd156799d1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carto-metro-couche-lignes\n",
      "collisions-tpg-avec-tiers\n",
      "montees-mensuelles-par-arret-par-ligne\n",
      "arrets\n",
      "montees-journalieres-par-arret-par-ligne-2019\n",
      "kilometres-produits-journaliers-par-ligne\n",
      "carto-metro-couche-point\n",
      "mn_montees-par-arret-par-ligne-par-tranchehoraire\n",
      "montees-par-arret-par-ligne\n",
      "frequentation-journaliere-par-tranche-horaire\n"
     ]
    }
   ],
   "source": [
    "#get dataset list\n",
    "API_endpoint_url = \"/catalog/datasets\"\n",
    "\n",
    "url = base_url + API_endpoint_url\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "for dataset in data[\"results\"]:\n",
    "    dataset_id = dataset.get(\"dataset_id\")\n",
    "    print(f\"{dataset_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa2e1a1-733b-4e66-9bef-7fca635f563e",
   "metadata": {},
   "source": [
    "I will use the datasets **\"arrets\"** (stops) and **\"montees-par-arret-par-ligne\"** (boardings per stop per line).\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f187a82-13c5-4616-b540-b63fcebec444",
   "metadata": {},
   "source": [
    "**arrets**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30ae9afe-1274-42c2-8a8d-90748e68c7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DOWNLOAD_DATA_TPG: \n",
    "    #get stops \n",
    "    dataset_id = \"arrets\"\n",
    "    API_endpoint_url = f\"/catalog/datasets/{dataset_id}/exports/json\"\n",
    "\n",
    "\n",
    "    params = {\n",
    "        \"limit\": -1,                 # max 10000 par appel\n",
    "        \"lang\": \"fr\",\n",
    "        \"timezone\": \"Europe/Zurich\"\n",
    "    }\n",
    "\n",
    "    url = base_url + API_endpoint_url\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    data = response.json()\n",
    "    print(f\"Nombre de lignes : {len(data)}\")\n",
    "\n",
    "\n",
    "    data_df = pd.DataFrame(data)\n",
    "    display(data_df.head())\n",
    "\n",
    "    # Save data to file\n",
    "    file_name = \"arrets.csv\"\n",
    "    data_df.to_csv(file_name, index=False)\n",
    "    \n",
    "    del response\n",
    "    del data\n",
    "    del data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda37c4a-566e-45ce-82fe-2016807e5ab1",
   "metadata": {},
   "source": [
    "**Les frequentations**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1510a42c-7c76-4da0-b5df-cde0a740f82e",
   "metadata": {},
   "source": [
    "I will need the data for all lines, and then for specific lines or stops.  \n",
    "I will download the data for all lines, as well as the data for line 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a36b6fc-a91b-4815-b5cb-3d7afe95445a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_data_TPG(filter_name, data_filter): \n",
    "    dataset_id = \"montees-par-arret-par-ligne\"\n",
    "    API_endpoint_url = f\"/catalog/datasets/{dataset_id}/exports/json\"\n",
    "\n",
    "    where_clause = f\"date <= '{date_max}'\"\n",
    "    if data_filter:\n",
    "        where_clause += f\" AND ligne in {data_filter}\"\n",
    "        \n",
    "    params = {\n",
    "        \"limit\": -1,                 # -1 no limit\n",
    "        \"lang\": \"fr\",\n",
    "        \"timezone\": \"Europe/Zurich\",\n",
    "        \"where\": where_clause\n",
    "    }\n",
    "\n",
    "    url = base_url + API_endpoint_url\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    data = response.json()\n",
    "    print(f\"Nombre de lignes : {len(data)}\")\n",
    "\n",
    "\n",
    "    data_df = pd.DataFrame(data)\n",
    "    # display(data_df.head())\n",
    "\n",
    "    # Save data to file\n",
    "    file_name = f\"frequentations_{filter_name}.csv\"\n",
    "    data_df.to_csv(file_name, index=False)\n",
    "    \n",
    "    del response\n",
    "    del data\n",
    "    del data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e094ed3d-9ed7-4f6b-9c6a-67d1607c6b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DOWNLOAD_DATA_TPG:\n",
    "    # download ligne 12\n",
    "    filter_name = \"ligne12\"\n",
    "    data_filter = \"('12')\"\n",
    "    download_data_TPG(filter_name, data_filter)\n",
    "    \n",
    "    # download Tram\n",
    "    # filter_name = \"tram\"\n",
    "    # data_filter = \"('12','14','15','17','18')\"\n",
    "    # download_data_TPG(filter_name, data_filter)\n",
    "    \n",
    "    # download All\n",
    "    filter_name = \"all\"\n",
    "    data_filter = None\n",
    "    download_data_TPG(filter_name, data_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2188efe3-9196-4168-bd0b-f6affff98cf1",
   "metadata": {},
   "source": [
    "If we look at the file sizes:  \n",
    "- **frequentations_ligne12.csv**: 11.1 MB  \n",
    "- **frequentations_tram.csv**: 64.4 MB  \n",
    "- **frequentations_all.csv**: 662.7 MB  \n",
    "\n",
    "And most importantly, it takes **30 minutes** to download *frequentations_all.csv*.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767ee0c2-633e-476e-a8be-69dd904e13d7",
   "metadata": {},
   "source": [
    "The TPG updated their data on **August 9**: the daily data for 2021 is no longer directly available, as it has been archived.  \n",
    "I kept my files from before August 2025 in order to upload them to the Zenodo platform.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b55d8de-519a-4488-85c0-5f91ee14887c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lecture de frequentations_all.2021.csv...\n",
      "Lecture de frequentations_all.2022.csv...\n",
      "Lecture de frequentations_all.2023.csv...\n",
      "Lecture de frequentations_all.2024.csv...\n",
      "Lecture de frequentations_all.2025.csv...\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "if DOWNLOAD_DATA_Zenodo : \n",
    "    FILES = {\n",
    "    \"arrets.csv\": \"https://zenodo.org/records/16880747/files/arrets.csv\",\n",
    "    \"frequentations_ligne12.csv\": \"https://zenodo.org/records/16880747/files/frequentations_ligne12.csv\",\n",
    "    \"frequentations_all.2021.csv\": \"https://zenodo.org/records/16880747/files/frequentations_all.2021.csv\",\n",
    "    \"frequentations_all.2022.csv\": \"https://zenodo.org/records/16880747/files/frequentations_all.2022.csv\",\n",
    "    \"frequentations_all.2023.csv\": \"https://zenodo.org/records/16880747/files/frequentations_all.2023.csv\",\n",
    "    \"frequentations_all.2024.csv\": \"https://zenodo.org/records/16880747/files/frequentations_all.2024.csv\",\n",
    "    \"frequentations_all.2025.csv\": \"https://zenodo.org/records/16880747/files/frequentations_all.2025.csv\",\n",
    "    \"meteo_daily.csv\": \"https://zenodo.org/records/16880747/files/meteo_daily.csv\",\n",
    "    \"meteo_hourly.csv\": \"https://zenodo.org/records/16880747/files/meteo_hourly.csv\",\n",
    "    }\n",
    "    \n",
    "    for name, url in FILES.items():\n",
    "        urllib.request.urlretrieve(url, name)\n",
    "        \n",
    "    #concat with chatGPT\n",
    "    recent = []\n",
    "    for y in (2021, 2022, 2023, 2024, 2025):\n",
    "        p = Path(f\"frequentations_all.{y}.csv\")  # fichier dans le répertoire courant\n",
    "        if p.exists():\n",
    "            print(f\"Lecture de {p}...\")\n",
    "            recent.append(pd.read_csv(p, low_memory=False))\n",
    "    \n",
    "    df = pd.concat(recent, ignore_index=True)\n",
    "    \n",
    "    # Save data to file\n",
    "    file_name = f\"frequentations_all.csv\"\n",
    "    df.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5292858-6ca6-4adf-a08c-e550efd90db0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Weather data with Open-Meteo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7a166f-8384-432b-abd5-34aadc5acc86",
   "metadata": {},
   "source": [
    "The MeteoSwiss website is currently being migrated to the OGD platform, and the data is not yet available.  \n",
    "So I will use the website [open-meteo.com](https://open-meteo.com).  \n",
    "\n",
    "Using https://www.gps-coordinates.net/, we can get the geographic coordinates of the city of Geneva:\n",
    "\n",
    "**DD (decimal degrees)**  \n",
    "Latitude: 46.2017559  \n",
    "Longitude: 6.1466014  \n",
    "\n",
    "**Lat, Long:** 46.2017559, 6.1466014  \n",
    "\n",
    "**DMS (degrees, minutes, seconds)**  \n",
    "Latitude: N 46° 12' 6.321''  \n",
    "Longitude: E 6° 8' 47.765''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96827e9c-ae23-4342-a4c8-1a925c4f7f35",
   "metadata": {},
   "source": [
    "Then we can query the Open-Meteo website using the coordinates of Geneva:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8760e03-035f-47bf-90a4-3d80995ce671",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DOWNLOAD_DATA_METEO:\n",
    "    base_url  = \"https://archive-api.open-meteo.com\"\n",
    "    API_endpoint_url = \"/v1/archive\"\n",
    "\n",
    "    params = {\n",
    "        \"latitude\": 46.2022,\n",
    "        \"longitude\": 6.1457,\n",
    "        \"start_date\": \"2021-08-01\",  \n",
    "        \"end_date\": date_max.split(\"T\")[0],\n",
    "        \"daily\": \"weather_code\",\n",
    "        \"hourly\": [\"temperature_2m\", \"precipitation\", \"weather_code\", \"wind_speed_10m\"],\n",
    "        \"timezone\": \"auto\"\n",
    "    }\n",
    "\n",
    "    url = base_url + API_endpoint_url\n",
    "    # Appel API\n",
    "    response = requests.get(url, params=params)\n",
    "    #response.raise_for_status()  # pour voir les erreurs HTTP\n",
    "    data = response.json()\n",
    "    \n",
    "    \n",
    "    # Extract daily data\n",
    "    daily_data = data.get(\"daily\", {})\n",
    "    daily_data_df = pd.DataFrame(daily_data)\n",
    "    print(\"Daily data :\")\n",
    "    display(daily_data_df.head())\n",
    "\n",
    "    # Extract hourly data\n",
    "    hourly_data = data.get(\"hourly\", {})\n",
    "    hourly_data_df = pd.DataFrame(hourly_data)\n",
    "    print(\"Hourly data:\")\n",
    "    display(hourly_data_df.head())\n",
    "\n",
    "    # Save data to file\n",
    "    file_name = \"meteo_daily.csv\"\n",
    "    daily_data_df.to_csv(file_name, index=False)\n",
    "\n",
    "    file_name = \"meteo_hourly.csv\"\n",
    "    hourly_data_df.to_csv(file_name, index=False)\n",
    "    \n",
    "    del response\n",
    "    del data, daily_data, hourly_data\n",
    "    del daily_data_df, hourly_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2698fc27-0131-4268-807f-07f06ba5157e",
   "metadata": {},
   "source": [
    "Clear variables to use a bit less memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4edb20d9-0a48-4465-b427-30a1eee7d14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee27b608-f30b-4f49-9e89-d553a0853525",
   "metadata": {},
   "source": [
    "## (b) Plan to manage and process the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65cbc4d-7fd6-44eb-8afb-45636bbea55f",
   "metadata": {},
   "source": [
    "## Dataset  \n",
    "\n",
    "I have TPG data by stop, by line, and by day.  \n",
    "I will first run an EDA on the stops to get familiar with the network.  \n",
    "\n",
    "Then, I will run an EDA on the ridership of one line — line 12 — which I know well as a user.  \n",
    "After that, I will extend the analysis to all lines.  \n",
    "\n",
    "For the weather data, I have both hourly and daily values.  \n",
    "I can combine the two datasets: TPG (daily) and weather (daily).  \n",
    "\n",
    "\n",
    "\n",
    "## Data Type  \n",
    "\n",
    "The time data needs to be converted to **Datetime**.  \n",
    "\n",
    "\n",
    "\n",
    "## Data Processing Steps  \n",
    "\n",
    "### Preliminary EDA  \n",
    "1. **Cleaning**  \n",
    "   - Handle missing values and duplicates  \n",
    "   - Data validation (features)  \n",
    "\n",
    "2. **Feature creation**  \n",
    "   - Network variables: type of vehicle  \n",
    "\n",
    "3. **Merging sources**  \n",
    "   - Join datasets (ridership, weather, calendar) using the date  \n",
    "\n",
    "### Advanced EDA  \n",
    "4. **Temporal analysis**  \n",
    "   - Time-related variables: day of the week, month, holidays, vacation periods  \n",
    "\n",
    "5. **Preparation for modeling**  \n",
    "   - Encode categorical variables  \n",
    "   - Normalize numerical variables if needed  \n",
    "   - Split data into training and test sets based on time  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d220a167-5dd6-49f6-bd9a-5d79d22d3cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae57c8d-f51a-4b76-a010-10c8f1927445",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:adsml] *",
   "language": "python",
   "name": "conda-env-adsml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
